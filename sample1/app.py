import streamlit as st
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from typing import List, Dict, Optional, Literal
import uuid
import datetime
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# LLMãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã‚’å®šç¾©
ModelType = Literal["gpt-4o", "gemini-2.0-pro", "claude-3-7-sonnet"]


class State(TypedDict):
    # Messages have the type "list". The `add_messages` function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]
    current_model: ModelType
    system_message: Optional[str]


# LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
def get_llm(model_type: ModelType):
    if model_type == "gpt-4o":
        return ChatOpenAI(model="gpt-4o", temperature=0.7)
    elif model_type == "gemini-2.0-pro":
        return ChatGoogleGenerativeAI(model="gemini-2.0-pro", temperature=0.7)
    elif model_type == "claude-3-7-sonnet":
        return ChatAnthropic(model="claude-3-7-sonnet-20250211", temperature=0.7)
    else:
        raise ValueError(f"ä¸æ˜ãªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")


from typing import List, Dict, Any, TypedDict, Literal, Optional
import os
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END

# å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šï¼ˆå®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯.envãƒ•ã‚¡ã‚¤ãƒ«ãªã©ã§ç®¡ç†ï¼‰
# os.environ["OPENAI_API_KEY"] = "your-openai-key"
# os.environ["GOOGLE_API_KEY"] = "your-google-key"
# os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-key"

# LLMãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã‚’å®šç¾©
ModelType = Literal["gpt-4o", "gemini-2.0-pro", "claude-3-7-sonnet"]

# ãƒãƒ£ãƒƒãƒˆçŠ¶æ…‹ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹
class ChatState(TypedDict):
    messages: List[Dict[str, Any]]
    current_model: ModelType
    system_message: Optional[str]

# LLMãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã™ã‚‹é–¢æ•°
def get_llm(model_type: ModelType):
    if model_type == "gpt-4o":
        return ChatOpenAI(model="gpt-4o", temperature=0.7)
    elif model_type == "gemini-2.0-pro":
        return ChatGoogleGenerativeAI(model="gemini-2.0-pro", temperature=0.7)
    elif model_type == "claude-3-7-sonnet":
        return ChatAnthropic(model="claude-3-7-sonnet-20250211", temperature=0.7)
    else:
        raise ValueError(f"ä¸æ˜ãªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {model_type}")

# LLMã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã—ã¦å¿œç­”ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def query_llm(state: ChatState):
    """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦LLMã«å•ã„åˆã‚ã›ã‚’è¡Œã†"""
    current_model = state["current_model"]
    llm = get_llm(current_model)
    
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’æº–å‚™
    messages = []
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Œã°è¿½åŠ 
    if state["system_message"]:
        messages.append(SystemMessage(content=state["system_message"]))
    
    # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
    for msg in state["messages"]:
        if msg["role"] == "human":
            messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "ai":
            messages.append(AIMessage(content=msg["content"]))
    
    # LLMã«å•ã„åˆã‚ã›
    response = llm.invoke(messages)
    
    # å¿œç­”ã‚’ã‚¹ãƒ†ãƒ¼ãƒˆã«è¿½åŠ 
    new_messages = state["messages"].copy()
    new_messages.append({
        "role": "ai",
        "content": response.content,
        "model": current_model
    })
    
    return {"messages": new_messages, "current_model": current_model, "system_message": state["system_message"]}



# --- Session State ---
if "sessions" not in st.session_state:
    st.session_state["sessions"] = {}

if "current_session" not in st.session_state:
    session_id = str(uuid.uuid4())
    st.session_state["current_session"] = session_id
    st.session_state["sessions"][session_id] = {
        "model": "gpt-3.5-turbo",
        "messages": [],
        "created_at": datetime.datetime.now()
    }

# --- UI Components ---
st.set_page_config(page_title="LangGraph Chat App", layout="wide")

# Sidebar for session and model selection
with st.sidebar:
    st.markdown("### ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸€è¦§")
    for sid, sess in st.session_state["sessions"].items():
        label = sess["created_at"].strftime("%Y-%m-%d %H:%M:%S")
        if st.button(label, key=sid):
            st.session_state["current_session"] = sid

    if st.button("â• æ–°ã—ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹"):
        new_id = str(uuid.uuid4())
        st.session_state["current_session"] = new_id
        st.session_state["sessions"][new_id] = {
            "model": "gpt-3.5-turbo",
            "messages": [],
            "created_at": datetime.datetime.now()
        }

    st.markdown("---")
    st.markdown("### ãƒ¢ãƒ‡ãƒ«é¸æŠ")
    model = st.selectbox("ä½¿ç”¨ã™ã‚‹LLMã‚’é¸æŠ", [
        "gpt-3.5-turbo", 
        "gpt-4o", 
        "claude-3.5", 
        "claude-3-opus-20240229", 
        "gemini-2.0pro"
    ])
    current_session = st.session_state["current_session"]
    st.session_state["sessions"][current_session]["model"] = model

    st.markdown("### ä¼šè©±å±¥æ­´")
    for msg in st.session_state["sessions"][current_session]["messages"]:
        st.markdown(f"**{msg['role'].capitalize()}**: {msg['content']}")

# --- LangGraph Setup ---
def llm_node(state: Dict) -> Dict:
    model_name = state["model"]
    input_text = state["messages"]

    if model_name.startswith("gpt"):
        llm = ChatOpenAI(model=model_name, temperature=0)
        messages = [HumanMessage(content=input_text)]
        response = llm(messages)
        return {"response": response.content}

    elif model_name.startswith("claude"):
        return {"response": f"(Claudeã«ã‚ˆã‚‹å¿œç­”ã®ãƒ¢ãƒƒã‚¯): {input_text}"}

    elif model_name.startswith("gemini"):
        return {"response": f"(Geminiã«ã‚ˆã‚‹å¿œç­”ã®ãƒ¢ãƒƒã‚¯): {input_text}"}

    return {"response": "ä¸æ˜ãªãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸã€‚"}

builder = StateGraph(State)
builder.add_node("llm", llm_node)
builder.set_entry_point("llm")
builder.set_finish_point("llm")
chat_graph = builder.compile()

# --- Chat UI ---
st.title("ğŸ§  LangGraph ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒª")

user_input = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›...")
if user_input:
    current_session = st.session_state["current_session"]
    model = st.session_state["sessions"][current_session]["model"]

    st.session_state["sessions"][current_session]["messages"].append({"role": "user", "content": user_input})

    with st.spinner("è€ƒãˆä¸­..."):
        state = {"input": user_input, "model": model}
        result = chat_graph.invoke(state)
        response = result["response"]

    st.session_state["sessions"][current_session]["messages"].append({"role": "assistant", "content": response})

# Display chat messages
for msg in st.session_state["sessions"][st.session_state["current_session"]]["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
